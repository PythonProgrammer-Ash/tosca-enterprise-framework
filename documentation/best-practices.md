Tosca Test Automation Best Practices
ğŸ“š Overview
This document outlines proven best practices for building and maintaining enterprise-grade Tosca test automation frameworks.

ğŸ—ï¸ Framework Architecture

1. Workspace Organization
   âœ… DO:
   âœ“ Use logical folder hierarchy (max 3-4 levels deep)
   âœ“ Group modules by application area/feature
   âœ“ Separate smoke, regression, and API tests
   âœ“ Keep workspace "slim" (< 20% redundancy)
   âœ“ Use consistent naming conventions

Example Structure:
Banking_App/
â”œâ”€â”€ Modules/
â”‚ â”œâ”€â”€ Login/
â”‚ â”œâ”€â”€ AccountManagement/
â”‚ â””â”€â”€ Transactions/
â”œâ”€â”€ TestCases/
â”‚ â”œâ”€â”€ Smoke/
â”‚ â”œâ”€â”€ Regression/
â”‚ â””â”€â”€ API/
â””â”€â”€ TestStepBlocks/
â”œâ”€â”€ Common/
â””â”€â”€ Domain_Specific/
âŒ DON'T:
âœ— Create deep folder nesting (> 5 levels)
âœ— Mix different test types in same folder
âœ— Duplicate modules or test logic
âœ— Use inconsistent naming patterns
âœ— Create "dump" folders for uncategorized items
ğŸ§© Module Design 2. XScan Strategy
âœ… DO:
âœ“ Scan complete application flows, not individual pages
âœ“ Use ID as primary steering parameter
âœ“ Configure 3-4 fallback steering parameters
âœ“ Enable adaptive identification for dynamic elements
âœ“ Group related controls in same module
âœ“ Use module folders to organize controls logically

Steering Priority:

1. ID (most stable)
2. Name (good for static elements)
3. CSS Selector (for modern SPAs)
4. XPath with anchors (last resort)
   âŒ DON'T:
   âœ— Scan entire application in one module
   âœ— Rely solely on XPath without anchors
   âœ— Use visual recognition as primary method
   âœ— Scan the same element multiple times
   âœ— Create modules with > 100 controls
5. Module Attributes
   âœ… DO:
   âœ“ Use explicit waits (WaitOn conditions)
   âœ“ Set appropriate timeout values (10-30 seconds)
   âœ“ Configure recovery scenarios for common errors
   âœ“ Use business-friendly control names
   âœ“ Document complex steering logic

Example:
Control: btn_SubmitTransaction
â”œâ”€â”€ WaitOn: Element.Exists AND Element.Enabled
â”œâ”€â”€ Timeout: 20 seconds
â”œâ”€â”€ Recovery: Handle_SessionTimeout
â””â”€â”€ Description: "Submits transaction after validation"
âŒ DON'T:
âœ— Use hardcoded Sleep() statements
âœ— Set excessive timeouts (> 60 seconds)
âœ— Ignore recovery scenarios
âœ— Use cryptic names like btn_1, txt_a
âœ— Leave controls undocumented
ğŸ§ª TestCase Design 4. Reusable TestStepBlocks (RTBs)
âœ… DO:
âœ“ Create RTBs for repeated actions (login, navigation)
âœ“ Use Business Parameters for data input
âœ“ Make RTBs generic and configurable
âœ“ Document RTB purpose and parameters
âœ“ Version control RTB changes
âœ“ Test RTBs independently before using

Example RTB Structure:
RTB_CreateCustomer
â”œâ”€â”€ Input Parameters:
â”‚ â”œâ”€â”€ BP[FirstName]
â”‚ â”œâ”€â”€ BP[LastName]
â”‚ â”œâ”€â”€ BP[Email]
â”‚ â””â”€â”€ BP[CustomerType]
â”œâ”€â”€ Logic:
â”‚ â”œâ”€â”€ Navigate to Customer Creation
â”‚ â”œâ”€â”€ Fill Form with parameters
â”‚ â”œâ”€â”€ Submit and Validate
â”‚ â””â”€â”€ Capture Customer ID â†’ Buffer
â””â”€â”€ Output:
â””â”€â”€ {Buffer[CustomerID]}
âŒ DON'T:
âœ— Hardcode data inside RTBs
âœ— Create RTBs with > 50 steps
âœ— Make RTBs too specific to one scenario
âœ— Nest RTBs more than 3 levels deep
âœ— Skip parameter validation 5. Test Data Management
âœ… DO:
âœ“ Use TCD (Test Case Design) for data-driven testing
âœ“ Leverage Template Instances for combinatorial tests
âœ“ Use TBox buffers to pass data between steps
âœ“ Store reusable data in Excel/Database
âœ“ Separate test data from test logic
âœ“ Use meaningful buffer names

Buffer Usage Pattern:
Step 1: Create Order
â”œâ”€â”€ TBox Set â†’ {Buffer[OrderID]} = "ORD-12345"
â””â”€â”€ TBox Set â†’ {Buffer[OrderAmount]} = "$500.00"

Step 2: Validate Order
â”œâ”€â”€ Input: {Buffer[OrderID]}
â””â”€â”€ Assert: Amount == {Buffer[OrderAmount]}
âŒ DON'T:
âœ— Hardcode test data in TestSteps
âœ— Mix production and test data
âœ— Use same data across parallel executions
âœ— Forget to clear buffers after use
âœ— Use generic buffer names (data1, temp)
ğŸ” Control Flow & Logic 6. Conditional Logic
âœ… DO:
âœ“ Use IF/ELSE for branching scenarios
âœ“ Implement WHILE loops with exit conditions
âœ“ Add safety counters to prevent infinite loops
âœ“ Use proper verification steps
âœ“ Log decision points for debugging

Example:
IF {Buffer[AccountStatus]} == "Active"
â”œâ”€â”€ THEN: Proceed with Transaction
â”‚ â”œâ”€â”€ RTB_ProcessPayment
â”‚ â””â”€â”€ RTB_GenerateReceipt
â””â”€â”€ ELSE: Send Notification
â”œâ”€â”€ RTB_SendAccountInactiveEmail
â””â”€â”€ Log: "Account inactive, transaction skipped"
âŒ DON'T:
âœ— Create deeply nested conditions (> 3 levels)
âœ— Use WHILE without exit conditions
âœ— Skip verification after conditional branches
âœ— Forget to handle ELSE cases
âœ— Use complex logical expressions without comments 7. Error Handling
âœ… DO:
âœ“ Implement recovery scenarios at module level
âœ“ Use try-catch patterns via TestStepBlocks
âœ“ Log errors with context information
âœ“ Take screenshots on failures
âœ“ Clean up test data even on failure
âœ“ Define clear error messages

Recovery Scenario Example:
On: Control Not Found Error
â”œâ”€â”€ Action 1: Wait 5 seconds
â”œâ”€â”€ Action 2: Refresh page
â”œâ”€â”€ Action 3: Re-scan control
â”œâ”€â”€ Action 4: Log error details
â””â”€â”€ Action 5: Take screenshot
âŒ DON'T:
âœ— Ignore errors and continue execution
âœ— Create generic "catch all" handlers
âœ— Skip cleanup on test failures
âœ— Use vague error messages
âœ— Let tests fail silently
âš¡ Performance & Scalability 8. Execution Optimization
âœ… DO:
âœ“ Use Distributed Execution (DEX) for parallel runs
âœ“ Configure appropriate wait strategies
âœ“ Minimize unnecessary verifications
âœ“ Use API calls for backend validation
âœ“ Implement smart synchronization
âœ“ Cache reusable data

Parallel Execution Strategy:
Suite: 250 tests
â”œâ”€â”€ DEX Agent 1: Tests 1-83 (Smoke + Critical)
â”œâ”€â”€ DEX Agent 2: Tests 84-166 (Regression Set A)
â””â”€â”€ DEX Agent 3: Tests 167-250 (Regression Set B)

Execution Time:
Sequential: 12.5 hours
Parallel (3 agents): 4.2 hours â†’ 70% reduction
âŒ DON'T:
âœ— Use Sleep() instead of intelligent waits
âœ— Run all tests sequentially when parallel is available
âœ— Over-verify (checking same thing multiple times)
âœ— Perform full UI flows when API suffices
âœ— Ignore execution time metrics 9. Maintenance Strategy
âœ… DO:
âœ“ Review and refactor test code quarterly
âœ“ Monitor test execution metrics
âœ“ Identify and fix flaky tests immediately
âœ“ Update modules when application changes
âœ“ Archive obsolete test cases
âœ“ Maintain test documentation

Maintenance Metrics to Track:
â”œâ”€â”€ Test Pass Rate (target: > 95%)
â”œâ”€â”€ Avg Execution Time (monitor trends)
â”œâ”€â”€ Flaky Test Rate (target: < 2%)
â”œâ”€â”€ Module Reusability (target: > 70%)
â””â”€â”€ Maintenance Time (hours/week)
âŒ DON'T:
âœ— Let flaky tests persist
âœ— Ignore slow-running tests
âœ— Keep obsolete test cases active
âœ— Delay module updates
âœ— Skip regular framework reviews
ğŸ”Œ Integration Best Practices 10. CI/CD Integration
âœ… DO:
âœ“ Automate test execution via ToscaCI
âœ“ Integrate with version control (Git)
âœ“ Configure quality gates (pass rate thresholds)
âœ“ Generate and publish test reports
âœ“ Send notifications on failures
âœ“ Maintain separate environments (DEV/QA/STAGE)

Jenkins Pipeline Pattern:
trigger: [Git push to main]
â”œâ”€â”€ Stage 1: Validate Environment
â”œâ”€â”€ Stage 2: Execute Smoke Tests
â”œâ”€â”€ Stage 3: Quality Gate Check
â”‚ â””â”€â”€ IF Pass Rate < 95% â†’ FAIL Pipeline
â”œâ”€â”€ Stage 4: Execute Regression
â”œâ”€â”€ Stage 5: Publish Results
â””â”€â”€ Stage 6: Notify Team
âŒ DON'T:
âœ— Manually trigger automated tests
âœ— Run tests against production environments
âœ— Skip quality gates to "pass" builds
âœ— Ignore failed pipeline notifications
âœ— Mix test environments 11. Test Management Integration
âœ… DO:
âœ“ Sync test cases with qTest/JIRA
âœ“ Map requirements to test cases
âœ“ Auto-create defects for failures
âœ“ Track test coverage metrics
âœ“ Maintain traceability matrix
âœ“ Update test status in real-time

Traceability Example:
Requirement: REQ-123 (Fund Transfer)
â”œâ”€â”€ Linked Tests:
â”‚ â”œâ”€â”€ TC-456: Valid Transfer
â”‚ â”œâ”€â”€ TC-457: Insufficient Funds
â”‚ â”œâ”€â”€ TC-458: Invalid Account
â”‚ â””â”€â”€ TC-459: Daily Limit Check
â””â”€â”€ Coverage: 100% (All scenarios automated)
âŒ DON'T:
âœ— Maintain duplicate test repositories
âœ— Skip requirement linkage
âœ— Manually log defects
âœ— Ignore coverage gaps
âœ— Let test status become stale
ğŸ“Š Reporting & Analytics 12. Reporting Standards
âœ… DO:
âœ“ Generate comprehensive execution reports
âœ“ Include screenshots for failures
âœ“ Track historical trends
âœ“ Provide actionable insights
âœ“ Customize reports for stakeholders
âœ“ Archive reports for compliance

Report Components:
â”œâ”€â”€ Executive Summary (pass/fail rate)
â”œâ”€â”€ Detailed Results (per test case)
â”œâ”€â”€ Failure Analysis (root causes)
â”œâ”€â”€ Trend Charts (30-day history)
â”œâ”€â”€ Coverage Metrics (requirements coverage)
â””â”€â”€ Performance Data (execution times)
âŒ DON'T:
âœ— Generate raw XML dumps as "reports"
âœ— Skip failure details
âœ— Ignore historical trends
âœ— Create one-size-fits-all reports
âœ— Delete old reports prematurely
ğŸ”’ Security & Compliance 13. Security Best Practices
âœ… DO:
âœ“ Use credential management tools (not hardcoded)
âœ“ Encrypt sensitive test data
âœ“ Implement role-based access control
âœ“ Audit test execution logs
âœ“ Secure CI/CD pipeline credentials
âœ“ Sanitize data in reports/screenshots

Credential Management:
â”œâ”€â”€ Use: Jenkins Credentials Plugin
â”œâ”€â”€ Or: Azure Key Vault
â”œâ”€â”€ Or: HashiCorp Vault
â””â”€â”€ Never: Hardcode in Tosca or scripts
âŒ DON'T:
âœ— Hardcode passwords in test cases
âœ— Commit credentials to Git
âœ— Share test accounts openly
âœ— Expose sensitive data in reports
âœ— Skip access control reviews
ğŸ“ Naming Conventions 14. Standardized Naming
âœ… DO:
Modules:
{Application}\_{Feature}\_Module
Example: Banking_Login_Module

TestCases:
{Process}\_{Scenario}\_TC
Example: FundTransfer_ValidAmount_TC

TestStepBlocks:
RTB*{Action}*{Object}
Example: RTB_Validate_AccountBalance

Buffers:
{Context}{DataType}
Example: OrderID, CustomerEmail, TransactionAmount

ExecutionLists:
{Project}\_{Type}\_Suite
Example: Banking_Regression_Suite
âŒ DON'T:
âœ— Module1, Test_A, RTB_123
âœ— tmp, data, value
âœ— Using spaces instead of underscores
âœ— Inconsistent capitalization
âœ— Cryptic abbreviations
ğŸ¯ Key Takeaways
Golden Rules:
Modularity: Build reusable components
Clarity: Use descriptive names and documentation
Stability: Implement robust steering and waits
Scalability: Design for parallel execution
Maintainability: Regular reviews and refactoring
Integration: Connect with ecosystem tools
Security: Protect credentials and sensitive data
Metrics: Track and improve continuously
Success Metrics:
Target Framework Health:
â”œâ”€â”€ Pass Rate: > 95%
â”œâ”€â”€ Flaky Tests: < 2%
â”œâ”€â”€ Execution Time: Optimized with DEX
â”œâ”€â”€ Reusability: > 70% (RTB usage)
â”œâ”€â”€ Coverage: > 85% (requirements)
â”œâ”€â”€ Maintenance: < 10 hours/week
â””â”€â”€ ROI: Measurable time/cost savings
ğŸ“š Additional Resources
Tricentis Best Practices (TBP)
Tosca Commander User Guide
Test Case Design Guide
Remember: Great automation is built on great architecture. Follow these practices to create maintainable, scalable, and reliable test automation frameworks.
